{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"background-color: lightgreen\"><font size =6>Programming / AI</font></span>\n",
    "* in the next two lessons, we will do a comprehensive research on US universities\n",
    "* step 1: starting from an entry page of us university list\n",
    "* step 2: loop it and use wikipedia summary to summarize them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To achieve our goals (aka pancakes you like), we need:**\n",
    "* which libraries (utensils, baker etc.) to use\n",
    "* how tools work\n",
    "* define steps and procedures\n",
    "* techniques of manipulating ingredients, sometime with tools, sometimes without"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's work on it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review the steps\n",
    "* try to visualize the end-to-end process\n",
    "\n",
    "**1. we open a brower <br>** \n",
    "**2. we type in the url we want to browse <br>**\n",
    "**3. we read it through, maybe also take some notes <br>**\n",
    "**4. we summarize in few words or sentences <br>**\n",
    "**5. repeat for other urls <br>** \n",
    "<br> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from IPython.display import Image, display, Markdown\n",
    "\n",
    "DOCUMENTS_PATH = os.getcwd()\n",
    "\n",
    "display(Markdown(\"**In a technical term, steps are:**\"))\n",
    "display(Image(f\"{DOCUMENTS_PATH}/standard_flow.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## which libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import re \n",
    "import requests \n",
    "import numpy as np\n",
    "import math\n",
    "from bs4 import BeautifulSoup  \n",
    "import pandas as pd \n",
    "from pathlib import Path\n",
    "import nltk \n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.tokenize import word_tokenize \n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image as img\n",
    "import time\n",
    "\n",
    "import wikipedia\n",
    "import wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4 color=red>Before Writing our first line of code</font><br>\n",
    "`go to the page` <br>\n",
    "`press control + U to view overall html (or json etc) file structure`<br>\n",
    "`back to the page, find out which part of the page we want to scrape`<br>\n",
    "`right click and choose inspect`<br>\n",
    "`you may need to go back and forth multiple times to confirm/double check`<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## college list\n",
    "* we are going to do some research on SAT test\n",
    "* we are using wikipedia \n",
    "* step 1: install wikipedia\n",
    "* brief check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### search for pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_local = False\n",
    "def get_entry_urls(url = None, filter_str = None):\n",
    "    \"\"\"\n",
    "        output: state name, category, url\n",
    "    \"\"\"\n",
    "    all_urls = []\n",
    "\n",
    "    if use_local: \n",
    "        url = r\"C:\\SP\\Teaching\\DSCamp\\University_List_EntryPage.html\"\n",
    "        with open(url, \"r\", encoding = \"utf8\") as fp:\n",
    "            soup = BeautifulSoup(fp, 'html.parser') \n",
    "    else:\n",
    "        url = \"https://en.wikipedia.org/wiki/Lists_of_American_universities_and_colleges\"\n",
    "        response = requests.get(url) \n",
    "        soup = BeautifulSoup(response.content, 'html.parser') \n",
    "\n",
    "    if not filter_str:\n",
    "        filter_str = \"List_of_colleges_and_universities_in\"\n",
    "        \n",
    "    for link in soup.findAll('a'): \n",
    "        sub_url = link.get(\"href\") #link[\"href\"]\n",
    "        if sub_url:\n",
    "            if re.search(filter_str, sub_url): \n",
    "                #print(link[\"title\"])\n",
    "                text = link.get_text() \n",
    "                if link.has_attr('title'):\n",
    "                    title = link[\"title\"]  \n",
    "\n",
    "                    all_urls.append([text, title, sub_url])\n",
    "    return all_urls "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_urls = get_entry_urls()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# now loop it\n",
    "* generate college url list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['New Jersey',\n",
       " 'List of colleges and universities in New Jersey',\n",
       " '/wiki/List_of_colleges_and_universities_in_New_Jersey']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_urls[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_url_list(all_urls): #all_urls\n",
    "    \"\"\"\n",
    "        output: state name, category, url\n",
    "    \"\"\"\n",
    "    colleges_urls = []\n",
    "    root = \"https://en.wikipedia.org/\"\n",
    "    ii = 0\n",
    "    for one_url in all_urls:  \n",
    "        State, Category, url = one_url[0], one_url[1], one_url[2] \n",
    "        state_url = root + url\n",
    "        \n",
    "        response = requests.get(state_url) \n",
    "        if response.status_code != 200:\n",
    "            continue\n",
    "        soup = BeautifulSoup(response.content, 'html.parser') \n",
    "\n",
    "        state_urls = []\n",
    "        for link in soup.findAll('a'):\n",
    "            college_url = link.get(\"href\") #link[\"href\"]\n",
    "            if college_url:\n",
    "                filter_str = \"Unive\"\n",
    "                if re.search(filter_str, college_url, re.IGNORECASE):  \n",
    "                    #print(link[\"title\"])\n",
    "                    text = link.get_text() \n",
    "                    if link.has_attr('title'):\n",
    "                        title = link[\"title\"]  \n",
    "\n",
    "                        state_urls.append([State, Category, url, text, title, college_url])\n",
    "        colleges_urls.extend(state_urls)\n",
    "        #ii += 1\n",
    "        #if ii > 2: \n",
    "        #    break\n",
    "        time.sleep(5)\n",
    " \n",
    "    df_college = pd.DataFrame(colleges_urls, columns = [\"State\", \"Category\", \"State College Url\", \"College Title\", \"Title 2\", \"College Url\"])\n",
    "    return df_college"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = scrape_url_list(all_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols.to_csv(r\"C:\\SP\\Teaching\\CrossLessons/USACollegeList.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Py Learning from Above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List: advanced \n",
    "## Exception (optional)\n",
    "## access a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p39tf",
   "language": "python",
   "name": "p39tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
